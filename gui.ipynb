# import dependencies
from sklearn import linear_model
import cv2
import tkinter as tk
from tkinter import filedialog
import tkinter.messagebox as mb
from PIL import Image, ImageTk
import matplotlib.pyplot as plt
import numpy as np
import torch
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
import sys
sys.path.append("..")
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor
import time
import random
import warnings
import re
#from transformers import TrOCRProcessor, VisionEncoderDecoderModel #load microsoft OCR module
from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition # load larger microsoft OCR
import requests
#warnings.filterwarnings("ignore")


# Global variable for current_embedding
global original_image, last_evaluation_time, mouse_positions,global_masks, scale_w, scale_h
global_masks            = []
mouse_positions         = []
input_points            = [] #initialize empty list
original_image          = None
current_point_label     = 1
last_evaluation_time    = time.time()
predictor               = None

# ROI
roi_start   = None
roi_end     = None
roi_mode    = False
rectangle_object = None

# Scaling of image
scale_w, scale_h = None,None

# Backend functions
def show_anns(anns):
    if len(anns) == 0:
        return
    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
    ax = plt.gca()
    ax.set_autoscale_on(False)
    polygons = []
    color = []
    for ann in sorted_anns:
        m = ann['segmentation']
        img = np.ones((m.shape[0], m.shape[1], 3))
        color_mask = np.random.random((1, 3)).tolist()[0]
        for i in range(3):
            img[:,:,i] = color_mask[i]
        ax.imshow(np.dstack((img, m*0.35)))

def load_model():
    sam_checkpoint = "sam_vit_b_01ec64.pth"
    model_type = "vit_b"
    device = "cuda"
    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
    sam.to(device=device)
    return SamPredictor(sam)


def eval_image(embedding, input_points, input_labels):
    global masks 
    masks, scores, logits = embedding.predict(
        point_coords=input_points,#np.array([[500, 1500],[700, 1500]]),#input_point,
        point_labels=input_labels,#np.array([1,1]),#input_label,
        multimask_output=False,
    )
    return masks,scores,logits




# GUI functions
def load_image(file_path):
    return cv2.cvtColor(cv2.imread(file_path, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH), cv2.COLOR_BGR2RGB)


def on_canvas_click(event):
    global input_points, current_point_label, roi_start, roi_end, roi_mode, rectangle_object, scale_w, scale_h

    # Rescale the click coordinates
    x, y = event.x * scale_w, event.y * scale_h

    if roi_mode:
        if roi_start is None:
            # Starting point
            roi_start = (x,y)
            print(f"ROI start: {roi_start}")
        else:
            # Ending point
            roi_end = (x,y)
            
            # register the two points
            x1, y1 = roi_start
            x2, y2 = roi_end
            # delete previous rectangle
            if rectangle_object:
                # Delete the previous rectangle
                canvas.delete(rectangle_object)

            rectangle_object = canvas.create_rectangle(x1, y1, x2, y2, outline='red')

            print(f"ROI start: {roi_start}")
            roi_mode = False # Switch off access to this loop
            # Process the ROI
            unit_p_px = process_roi()

    else:
        if predictor is not None:
            #x, y = event.x, event.y
            point_coords = np.array([[x , y ]])  # remember, 4 is the shrink factor of the image in the gui
            input_points.append((point_coords,current_point_label))

            # Print the input points
            print(f"Input point: {input_points}")

            # Draw a dot at the click location
            radius = 2
            #canvas.create_oval(x - radius, y - radius, x + radius, y + radius, fill='red')


def on_canvas_motion(event):
    global input_points, last_evaluation_time, mouse_positions, scale_h, scale_w
    current_time = time.time()

    x, y = event.x * scale_w, event.y * scale_h
    
    #coords_label.place(x=x, y=y)  # Update the label's position
    #coords_label.config(text=f"({x}, {y})")  # Update the label's text

    if predictor is not None and current_time - last_evaluation_time > 0.1:
        point_coords = np.array([[x, y]])   # remember, 3 is the shrink factor of the image in the gui   (WARNING: BEHAVIOR IS EXTREMELY WEIRD, SCALE_W SHOULD BE SCALE_H AND SO ON BUT IT WORKS WITHOUT MAKING THAT CHANGE)
        #input_points.append((point_coords, 1))  # Append the current point to input_points
        mouse_positions.append(point_coords)  # Append the current point to mouse_positions

        evaluate_points()
        last_evaluation_time = current_time  # Update the last_evaluation_time



def toggle_roi_mode():
    global roi_mode, roi_start, roi_end
    roi_mode = not roi_mode  # Toggle ROI mode
    roi_start = None  # Reset the ROI
    roi_end = None
    if roi_mode:
        print("ROI selection mode enabled.")
    else:
        print("ROI selection mode disabled.")


def process_roi():
    global roi_start, roi_end, original_image
    # Step 1: Take point data and extract ROI
    data = [tuple(3*x for x in roi_start), tuple(3*x for x in roi_end)]
    x_values = [t[0] for t in data]
    y_values = [t[1] for t in data]
    roi_image = original_image[y_values[0]:y_values[1], x_values[0]:x_values[1]]

    # Step 2: Calculate length of the line using connected region analysis
    _, binary = cv2.threshold(roi_image, 127, 255, cv2.THRESH_BINARY_INV)
    binary = cv2.Canny(roi_image,  100, 1000)
    num_labels, labels = cv2.connectedComponents(binary)
    component_areas = []
    for label in range(1, num_labels):
        component_area = np.sum(labels == label)
        component_areas.append((label, component_area))
    component_areas.sort(key=lambda x: x[1], reverse=True)
    line_label = component_areas[0][0]
    line_image = np.zeros_like(labels,dtype=np.uint8)
    line_image[labels == line_label] = 255
    lines = cv2.HoughLinesP(line_image, 1, np.pi/180, 50, maxLineGap=50)
    counter = 0
    length = 0
    for line in lines:
        for x1, y1, x2, y2 in line:
            length_max = length
            length = np.sqrt((x2-x1)**2 + (y2-y1)**2)
            if length_max < length:
                length_max = length
            counter +=1
            print(f"Length of line is: {length} pixels")

    # Step 3: Apply OCR to detect text
    temp = np.where(line_image > 0) # Find index of maximum horizontal pixel in  line_image. The numbers are in the part of the image following this.
    image = roi_image[:,np.max(temp[1]):-1] # Subsample the roi_image

    # Apply Microsoft OCR, large model, as it performs better on decimal point detection
    processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')
    model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')
    pixel_values = processor(images=image, return_tensors="pt").pixel_values
    generated_ids = model.generate(pixel_values)
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]

    # Calculate the units/pixel value
    matches = re.findall(r'\d+\.\d+', generated_text)
    unit_p_px = (float(matches[0])/length_max)

    status_label.config(text = f"Units per pixel: {unit_p_px}")
    print(f"Units per pixel: {unit_p_px}")
    return unit_p_px







        
def toggle_point_label():
    global current_point_label
    current_point_label = 1 - current_point_label  # Toggle between 0 and 1
    toggle_button.config(text=f"Toggle Point Label (Current: {current_point_label})")




# Update the evaluate_points function to support the toggling of input points
def evaluate_points():
    global input_points, current_image, original_image, mouse_positions, masks
    
    if predictor is not None and (len(input_points) > 0 or len(mouse_positions) > 0):
        current_image = original_image.copy()
        status_label.config(text="Evaluating the image...")


        if len(input_points) > 0:  # Add this check
            # Use the input points and labels from the input_points list
            input_points_array = np.vstack([point[0] for point in input_points])
            point_labels = np.array([point[1] for point in input_points])
        else:
            input_points_array = []
            point_labels = []


        if len(mouse_positions) > 0:
            input_points_array = np.vstack([input_points_array, np.vstack(mouse_positions)])
            point_labels = np.hstack([point_labels, np.ones(len(mouse_positions))])


        masks, scores, logits = eval_image(predictor, input_points_array, point_labels)
        


        # Display the last mask
        if len(masks) > 0:
            mask = masks[-1]
            mask_uint8 = (mask * 255).astype(np.uint8)

            # Change the mask color
            color = np.random.random((1, 3)).tolist()[0]
            colored_mask = np.stack([mask_uint8 * c for c in color], axis=-1).astype(np.uint8)

            mask_overlay = cv2.addWeighted(current_image, 0.7, colored_mask, 0.3, 0)
            display_image(mask_overlay)
            
        mouse_positions = []  # Reset input_points for the next evaluation



def add_image_to_sidebar(image_path, image):
    thumbnail = ImageTk.PhotoImage(Image.fromarray(image).resize((100, 100), Image.ANTIALIAS))
    label = tk.Label(image_list_frame, image=thumbnail)
    label.image = thumbnail
    label.pack(side=tk.TOP, padx=5, pady=5)


def open_image():
    global status_label, root, predictor,original_image#, current_embedding
    file_path = filedialog.askopenfilename()
    if file_path:
        status_label.config(text="Loading image...")
        root.update_idletasks()
        image = load_image(file_path)
        
        original_image = image.copy()

        status_label.config(text="Generating embedding...")
        root.update_idletasks()
        #predictor.set_image(img)
        predictor.set_image(image)

        #current_embedding = generate_embedding(predictor, image)
        


        add_image_to_sidebar(file_path, image)
        display_image(image)
        status_label.config(text="Image loaded and embedding generated.")

def display_image(image):
    global current_image, photo
    current_image = image

    # Resize the image to fit the canvas
    resize_and_draw()

    # Bind a function to resize the image every time the window size changes
    canvas.bind('<Configure>', lambda event: resize_and_draw())


def resize_and_draw():
    global current_image, photo, scale_w, scale_h

    # Get the new size of the canvas
    width = canvas.winfo_width()
    height = canvas.winfo_height()

    # Get the width and height of the original image
    height_orig, width_orig = current_image.shape[:2]

    # Calculate the display scaling and store it for rescaling all other points
    scale_w, scale_h = width_orig/width, height_orig/height

    # Resize the image to fit the canvas
    resized_image = cv2.resize(current_image, (width, height))

    # Clear the canvas
    canvas.delete("all")

    # Convert to PIL Image and then to ImageTk
    pil_image = Image.fromarray(resized_image)
    photo = ImageTk.PhotoImage(pil_image)

    # Draw the image on the canvas
    canvas.create_image(0, 0, anchor=tk.NW, image=photo)



def display_image_temp(image):
    global current_image, photo
    current_image = image



    # Resize the image to half the dimensions
    height, width = current_image.shape[:2]
    resized_image = cv2.resize(current_image, (width // 3, height // 3))

    


    # Clear the canvas
    canvas.delete("all")

    photo = ImageTk.PhotoImage(Image.fromarray(resized_image))
    canvas.create_image(0, 0, anchor=tk.NW, image=photo)



def clear_input_points():
    global input_points, current_image, original_image,mouse_positions,global_masks,masks
    input_points = []  # Clear input points
    mouse_positions = []
    #global_masks.append(masks)
    display_image(original_image)  # Reset the displayed image


def on_accept_points_click_old():
    global input_points, original_image, global_masks, masks

    # Append the current masks to the global_masks list
    global_masks.extend(masks)

    # Update the displayed image with the new masks
    masked_image = overlay_masks(original_image, global_masks)
    display_image(masked_image)

    # Reset the input points
    input_points = []
    mouse_positions = []

def overlay_masks(image, global_masks):
    # Create an empty mask with the same shape as the input image
    overlay = np.zeros_like(image)

    # Iterate through the global masks
    for mask in global_masks:
        
        # Generate a random color for the current mask
        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))

        # Create a colored mask by multiplying the current mask with the random color
        colored_mask = np.zeros_like(image)
        print(np.shape(mask),np.shape(image))
        for i in range(3):  # Iterate through color channels
            #print(np.shape(mask[0,...]),np.shape(colored_mask[...,i]))
            colored_mask[..., i] = mask[0,...] * color[i]

        # Add the colored mask to the overlay
        overlay = cv2.add(overlay, colored_mask)

    # Blend the input image with the overlay using a weight factor
    alpha = 0.5
    blended_image = cv2.addWeighted(image, 1 - alpha, overlay, alpha, 0)

    return blended_image
# How to use above function
#ablended_image = overlay_masks(original_image,global_masks)
#plt.imshow(ablended_image)


def on_accept_points_click():
    global input_points, original_image, global_masks, mouse_positions

    # Generate a mask based on the current input points
    if len(input_points) > 0:
        input_points_array = np.vstack([point[0] for point in input_points])
        point_labels = np.array([point[1] for point in input_points])
        
        masks, _, _ = eval_image(predictor, input_points_array, point_labels)

        # Append the current mask to the global masks list
        global_masks.extend(masks)

        # Update the displayed image with the new mask
        masked_image = original_image.copy()
        for mask in global_masks:
            mask_uint8 = (mask * 255).astype(np.uint8)

            # Change the mask color
            color = np.random.random((1, 3)).tolist()[0]
            colored_mask = np.stack([mask_uint8 * c for c in color], axis=-1).astype(np.uint8)

            masked_image = cv2.addWeighted(masked_image, 0.7, colored_mask, 0.3, 0)

        display_image(masked_image)

        # Reset the input points
        input_points = []
        mouse_positions = []



def load_model_and_predictor_temp():
    global predictor, sam
    sam_checkpoint = "sam_vit_b_01ec64.pth"
    model_type = "vit_b"
    #sam_checkpoint = "sam_vit_h_4b8939.pth"#"sam_vit_b_01ec64.pth"
    #model_type = "vit_h"#"vit_b"
    #sam_checkpoint = "sam_vit_l_0b3195.pth"#"sam_vit_b_01ec64.pth"
    #model_type = "vit_l"#"vit_b"
    
    device = "cuda"

    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
    sam.to(device=device)

    predictor = SamPredictor(sam) # If you want to go prompt based mask generation
    mask_generator = SamAutomaticMaskGenerator(sam) # If you want to to automatic mask generation
    
    status_label.config(text="Prompt based model loaded and predictor initialized.")
    #status_label.config(text="Auto-segmentation model loaded and predictor initialized.")



def autoseg(): #Using automatic segmentation
    
    return


def load_model_and_predictor():
    global predictor, sam, model_var, eval_var
    model_type = model_var.get()
    evaluation_type = eval_var.get()

    # Use a dictionary to map model_type to checkpoint names
    checkpoint_dict = {"vit_b": "sam_vit_b_01ec64.pth", 
                       "vit_l": "sam_vit_l_0b3195.pth", 
                       "vit_h": "sam_vit_h_4b8939.pth"}

    sam_checkpoint = checkpoint_dict[model_type]

    device = "cuda"

    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
    sam.to(device=device)

    if evaluation_type == 'Prompt based':
        predictor = SamPredictor(sam)
        status_label.config(text="Prompt based model loaded and predictor initialized.")
    else:
        predictor = SamAutomaticMaskGenerator(sam)
        status_label.config(text="Auto-segmentation model loaded and predictor initialized.")

def on_canvas_right_click(event):
    # Show the context menu
    context_menu.post(event.x_root, event.y_root)



def circle_fit(points):
    # Convert to float
    points = points.astype(float)
    
    # Compute the centroid
    centroid = np.mean(points, axis=0)
    
    # Move points to origin
    points -= centroid
    
    # Compute norm
    norms = np.linalg.norm(points, axis=1)
    
    # Solve linear least squares
    a, b, c = np.linalg.lstsq(np.c_[points, norms**2], np.ones(len(points)), rcond=None)[0]
    
    # Compute the circle parameters
    center = a / 2, b / 2
    radius = np.sqrt(c + center[0]**2 + center[1]**2)
    
    return np.array(center) + centroid, radius

def fit_circle_to_mask():
    global global_masks

    # Check if there are any masks
    if len(masks) > 0:
        mask = masks[-1]

        # Convert the mask to uint8
        mask_uint8 = (mask * 255).astype(np.uint8)

        # Find contours in the mask
        contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        # Find the contour with the maximum area (assuming this is the mask you want)
        max_contour = max(contours, key=cv2.contourArea)
        
        # Fit a circle using RANSAC
        best_inliers = None
        best_center = None
        best_radius = None
        for _ in range(100):  # Number of iterations
            
            # Select 3 random points
            random_points = random.sample(list(max_contour), 3)
            
            # Fit a circle to these points
            center, radius = circle_fit(np.array(random_points).reshape(-1, 2))
            
            # Compute distances to the circle
            distances = np.sqrt(np.sum((max_contour.reshape(-1, 2) - center)**2, axis=1)) - radius
            
            # Count inliers
            inliers = np.abs(distances) < 5  # Change the threshold if necessary
            
            if best_inliers is None or np.sum(inliers) > np.sum(best_inliers):
                best_inliers = inliers
                best_center = center
                best_radius = radius

        # Create a copy of the current image to draw on
        image_copy = current_image.copy()
        
        print(best_center, best_radius)
        # Draw the circle on the image
        # Fit a circle to the contour
        (x, y), radius = cv2.minEnclosingCircle(max_contour)
        center = (int(x), int(y))
        radius = int(radius)
        print(center,radius)
        cv2.circle(image_copy, center, radius, (0, 255, 0), 2)
        cv2.circle(image_copy, tuple(map(int, best_center)), int(best_radius), (0, 255, 0), 2)

        # Display the image
        display_image(image_copy)

# Initialize GUI
root = tk.Tk()
root.title("General Image Processor")
# Set a minimum size for the window
root.minsize(700, 600)

# Create and place the widgets
frame = tk.Frame(root)
frame.pack()

if False:
    # Add the load model button
    load_model_button = tk.Button(frame, text="Load Model", command=load_model_and_predictor)
    load_model_button.grid(row=0, column=0)


# Global variables for models and types
model_var = tk.StringVar()
eval_var = tk.StringVar()

# Modify button command
load_model_button = tk.Button(frame, text="Load Model", command=load_model_and_predictor)
load_model_button.grid(row=0, column=0)

# Create OptionMenu for models
model_options = ["vit_b", "vit_l", "vit_h"]
model_var.set(model_options[0])  # set the default option
model_menu = tk.OptionMenu(frame, model_var, *model_options)
model_menu.grid(row=0, column=1)

# Create OptionMenu for evaluations
eval_options = ["Prompt based", "Automatic"]
eval_var.set(eval_options[0])  # set the default option
eval_menu = tk.OptionMenu(frame, eval_var, *eval_options)
eval_menu.grid(row=0, column=2)


# Menu
menu = tk.Menu(root)
root.config(menu=menu)
file_menu = tk.Menu(menu)

status_label = tk.Label(root, text="")
status_label.pack()

menu.add_cascade(label="File", menu=file_menu)
file_menu.add_command(label="Open Image", command=open_image)

# Label toggle button
toggle_button = tk.Button(frame, text=f"Toggle Point Label (Current: {current_point_label})", command=toggle_point_label)
toggle_button.grid(row=0, column=3)

# Add an ROI mode toggle button
roi_mode_button = tk.Button(frame, text="Toggle ROI Mode", command=toggle_roi_mode)
roi_mode_button.grid(row=0, column=6)

# Image list frame (sidebar)
image_list_frame = tk.Frame(root)
image_list_frame.pack(side=tk.LEFT, fill=tk.Y)

if False:
    # Add the Evaluate button
    evaluate_button = tk.Button(frame, text="Evaluate", command=evaluate_points)
    evaluate_button.grid(row=0, column=2)


# Canvas (main area)
canvas = tk.Canvas(root, width=500, height=700, bg="white")
canvas.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True)

#coords_label = tk.Label(canvas, text="", bg="white")
#coords_label.pack()
## Pixel position label of the events
#coords_label = tk.Label(canvas, text="", bg="white")
#canvas.create_window(0, 0, anchor=tk.NW, window=coords_label)  # Use create_window instead of pack

# Add a clear button
clear_button = tk.Button(frame, text="Clear Points", command=clear_input_points)
clear_button.grid(row=0, column=4)

# Add an accept button
accept_button = tk.Button(frame, text="Accept Mask", command=on_accept_points_click)
accept_button.grid(row=0, column=5)

# Add an autosegment button
#autoseg_button = tk.Button(frame, text="Autosegment", command=autoseg_button_click)
#autoseg_button.grid(row=0, column=6)


# Create a context menu
context_menu = tk.Menu(root, tearoff=0)
context_menu.add_command(label="Fit Circle", command=fit_circle_to_mask)



canvas.bind("<Button-1>", on_canvas_click)
canvas.bind("<Motion>", on_canvas_motion)
canvas.bind("<Button-3>", on_canvas_right_click)  # Button-3 is the right click event


current_image = None
photo = None

root.mainloop()
